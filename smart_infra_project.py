# -*- coding: utf-8 -*-
"""Smart_Infra_Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sm2EgCizeRxsqtn0P2s6Mabh2Y80DOBv

### Libraries
"""

!pip install tensorflow

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler
import tensorflow as tf
import math
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

"""### Data-Preprocessing"""

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# 1. DATA LOADING AND EXPLORATION
# Load the data from URL
url = "https://docs.google.com/spreadsheets/d/1AYRqWP4AjJzR4TaERFq9XF2fiZi4tKOVyCUCYkuY3QQ/export?format=csv"
df = pd.read_csv(url)

# Display information about the dataset
print("Dataset Info:")
df.info()

print("\nFirst 5 rows:")
print(df.head())

# Find the target column (the one with 'compressive strength' in the name)
target_column = None
for col in df.columns:
    if 'compressive strength' in col.lower():
        target_column = col
        print(f"\nTarget column identified: {target_column}")
        break

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Explore relationships between features and target
plt.figure(figsize=(12, 10))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()

# Separate features and target
X = df.drop(target_column, axis=1)
y = df[target_column]

# Split the data into training, validation, and test sets (70%-15%-15%)
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.176, random_state=42) # 0.176 of 85% is ~15% of the total

# Normalize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print(f"Training set size: {X_train.shape}")
print(f"Validation set size: {X_val.shape}")
print(f"Test set size: {X_test.shape}")

"""### Model-Traning"""

# 3. BASELINE ANN MODEL
def create_baseline_model(input_dim):
    model = Sequential([
        Dense(64, activation='relu', input_dim=input_dim),
        Dense(32, activation='relu'),
        Dense(1, activation='linear')  # Linear activation for regression
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
    return model

# Create and train baseline model
baseline_model = create_baseline_model(X_train_scaled.shape[1])
print("\nBaseline Model Summary:")
baseline_model.summary()

# Set up callbacks for early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=30, verbose=1, restore_best_weights=True)

# Train the baseline model
baseline_history = baseline_model.fit(
    X_train_scaled, y_train,
    epochs=200,
    batch_size=32,
    validation_data=(X_val_scaled, y_val),
    callbacks=[early_stopping],
    verbose=1
)

# Evaluate baseline model
baseline_loss = baseline_model.evaluate(X_test_scaled, y_test)
baseline_mse = baseline_loss[0]
baseline_mae = baseline_loss[1]

# Plot training history and predictions in a single row
plt.figure(figsize=(18, 5))

# Training and Validation Loss plot
plt.subplot(1, 3, 1)
plt.plot(baseline_history.history['loss'], label='Training Loss')
plt.plot(baseline_history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()

# Training and Validation MAE plot
plt.subplot(1, 3, 2)
plt.plot(baseline_history.history['mae'], label='Training MAE')
plt.plot(baseline_history.history['val_mae'], label='Validation MAE')
plt.title('Training and Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('Mean Absolute Error')
plt.legend()

# Predictions vs Actual Values plot
plt.subplot(1, 3, 3)
baseline_predictions = baseline_model.predict(X_test_scaled).flatten()
plt.scatter(y_test, baseline_predictions, alpha=0.7)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.title('Actual vs Predicted')
plt.xlabel('Actual Strength (MPa)')
plt.ylabel('Predicted Strength (MPa)')

# Add space between subplots and save
plt.tight_layout()
plt.show()

# Calculate baseline R-squared
baseline_r2 = r2_score(y_test, baseline_predictions)
print(f"Baseline Model - Test MSE: {baseline_mse:.4f}")
print(f"Baseline Model - Test MAE: {baseline_mae:.4f}")
print(f"Baseline Model R²: {baseline_r2:.4f}")

"""### HYPERPARAMETER OPTIMIZATION"""

# Define different learning rate schedules
def step_decay(epoch):
    initial_lr = 0.001
    drop = 0.5
    epochs_drop = 20
    lr = initial_lr * math.pow(drop, math.floor((1 + epoch) / epochs_drop))
    return lr

def exponential_decay(epoch):
    initial_lr = 0.001
    k = 0.1
    lr = initial_lr * math.exp(-k * epoch)
    return lr

def cosine_annealing(epoch):
    initial_lr = 0.001
    min_lr = 0.00001
    max_epochs = 200
    return min_lr + (initial_lr - min_lr) * (1 + math.cos(math.pi * epoch / max_epochs)) / 2

# Create model variations for hyperparameter optimization
def create_model(n_layers, n_neurons, activation, optimizer_name, lr=0.001):
    model = Sequential()

    # Input layer
    model.add(Dense(n_neurons[0], activation=activation, input_dim=X_train_scaled.shape[1]))

    # Hidden layers
    for i in range(1, n_layers):
        model.add(Dense(n_neurons[i], activation=activation))
        model.add(Dropout(0.2))  # Add dropout for regularization

    # Output layer
    model.add(Dense(1, activation='linear'))

    # Set optimizer
    if optimizer_name == 'adam':
        optimizer = Adam(learning_rate=lr)
    elif optimizer_name == 'sgd':
        optimizer = SGD(learning_rate=lr, momentum=0.9)
    elif optimizer_name == 'rmsprop':
        optimizer = RMSprop(learning_rate=lr)

    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

# Define hyperparameter combinations to try (reduced for demonstration)
hyperparameter_combinations = [
    # n_layers, n_neurons, activation, optimizer_name, lr_schedule
    (2, [64, 32], 'relu', 'adam', None),
    (3, [128, 64, 32], 'relu', 'adam', None),
    (3, [128, 64, 32], 'tanh', 'adam', None),
    (3, [128, 64, 32], 'relu', 'sgd', None),
    (3, [128, 64, 32], 'relu', 'adam', 'step'),
    (3, [128, 64, 32], 'relu', 'adam', 'cosine')
]

# Train and evaluate different models
model_results = []

for i, (n_layers, n_neurons, activation, optimizer_name, lr_schedule) in enumerate(hyperparameter_combinations):
    print(f"\nTraining Model {i+1}/{len(hyperparameter_combinations)}")
    print(f"Configuration: layers={n_layers}, neurons={n_neurons}, activation={activation}, optimizer={optimizer_name}, lr_schedule={lr_schedule}")

    # Create model
    model = create_model(n_layers, n_neurons, activation, optimizer_name)

    # Set up callbacks
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=30, verbose=1, restore_best_weights=True)
    ]

    # Add learning rate scheduler if specified
    if lr_schedule == 'step':
        callbacks.append(LearningRateScheduler(step_decay))
    elif lr_schedule == 'exponential':
        callbacks.append(LearningRateScheduler(exponential_decay))
    elif lr_schedule == 'cosine':
        callbacks.append(LearningRateScheduler(cosine_annealing))

    # Train model with different batch sizes (reduced for demonstration)
    batch_sizes = [32, 64]
    batch_results = []

    for batch_size in batch_sizes:
        print(f"Training with batch size: {batch_size}")
        history = model.fit(
            X_train_scaled, y_train,
            epochs=150,  # Reduced for demonstration
            batch_size=batch_size,
            validation_data=(X_val_scaled, y_val),
            callbacks=callbacks,
            verbose=1
        )

        # Evaluate model
        val_loss = model.evaluate(X_val_scaled, y_val)
        val_mse = val_loss[0]
        val_mae = val_loss[1]
        print(f"Validation MSE: {val_mse:.4f}, Validation MAE: {val_mae:.4f}")

        batch_results.append((batch_size, val_mse, val_mae, history))

    # Get best batch size based on validation MSE
    best_batch = min(batch_results, key=lambda x: x[1])

    # Evaluate on test set with best batch size
    test_loss = model.evaluate(X_test_scaled, y_test)
    test_mse = test_loss[0]
    test_mae = test_loss[1]

    # Make predictions and calculate R²
    test_predictions = model.predict(X_test_scaled).flatten()
    test_r2 = r2_score(y_test, test_predictions)

    # Store results
    model_results.append({
        'config': {
            'n_layers': n_layers,
            'n_neurons': n_neurons,
            'activation': activation,
            'optimizer': optimizer_name,
            'lr_schedule': lr_schedule,
            'batch_size': best_batch[0]
        },
        'validation_mse': best_batch[1],
        'validation_mae': best_batch[2],
        'test_mse': test_mse,
        'test_mae': test_mae,
        'test_r2': test_r2,
        'history': best_batch[3]
    })

# Identify the best model based on validation MSE
best_model_idx = min(range(len(model_results)), key=lambda i: model_results[i]['validation_mse'])
best_model_config = model_results[best_model_idx]['config']

print("\nBest Model Configuration:")
for key, value in best_model_config.items():
    print(f"{key}: {value}")
print(f"Validation MSE: {model_results[best_model_idx]['validation_mse']:.4f}")
print(f"Validation MAE: {model_results[best_model_idx]['validation_mae']:.4f}")
print(f"Test MSE: {model_results[best_model_idx]['test_mse']:.4f}")
print(f"Test MAE: {model_results[best_model_idx]['test_mae']:.4f}")
print(f"Test R²: {model_results[best_model_idx]['test_r2']:.4f}")

"""### FINAL MODEL TRAINING AND EVALUATION"""

# Create and train the best model with the optimal configuration
final_model = create_model(
    best_model_config['n_layers'],
    best_model_config['n_neurons'],
    best_model_config['activation'],
    best_model_config['optimizer']
)

callbacks = [
    EarlyStopping(monitor='val_loss', patience=30, verbose=1, restore_best_weights=True)
]

if best_model_config['lr_schedule'] == 'step':
    callbacks.append(LearningRateScheduler(step_decay))
elif best_model_config['lr_schedule'] == 'exponential':
    callbacks.append(LearningRateScheduler(exponential_decay))
elif best_model_config['lr_schedule'] == 'cosine':
    callbacks.append(LearningRateScheduler(cosine_annealing))

print("\nTraining Final Model with Best Configuration:")
final_history = final_model.fit(
    X_train_scaled, y_train,
    epochs=200,
    batch_size=best_model_config['batch_size'],
    validation_data=(X_val_scaled, y_val),
    callbacks=callbacks,
    verbose=1
)

# Evaluate final model
y_pred = final_model.predict(X_test_scaled).flatten()
final_mse = mean_squared_error(y_test, y_pred)
final_rmse = np.sqrt(final_mse)
final_mae = mean_absolute_error(y_test, y_pred)
final_r2 = r2_score(y_test, y_pred)

print("\nFinal Model Performance:")
print(f"Mean Squared Error (MSE): {final_mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {final_rmse:.4f}")
print(f"Mean Absolute Error (MAE): {final_mae:.4f}")
print(f"R²: {final_r2:.4f}")

# Plot all three visualizations in a single line
plt.figure(figsize=(18, 5))

# Training and Validation Loss plot
plt.subplot(1, 3, 1)
plt.plot(final_history.history['loss'], label='Training Loss')
plt.plot(final_history.history['val_loss'], label='Validation Loss')
plt.title('Final Model: Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()

# Training and Validation MAE plot
plt.subplot(1, 3, 2)
plt.plot(final_history.history['mae'], label='Training MAE')
plt.plot(final_history.history['val_mae'], label='Validation MAE')
plt.title('Final Model: Validation MAE')
plt.xlabel('Epoch')
plt.ylabel('Mean Absolute Error')
plt.legend()

# Predictions vs Actual Values plot
plt.subplot(1, 3, 3)
plt.scatter(y_test, y_pred, alpha=0.7)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.title('Final Model: Actual vs Predicted')
plt.xlabel('Actual Strength (MPa)')
plt.ylabel('Predicted Strength (MPa)')

# Add spacing between subplots and save
plt.tight_layout()
plt.show()

# Feature importance analysis
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Weight': np.abs(final_model.layers[0].get_weights()[0].mean(axis=1))
})
feature_importance = feature_importance.sort_values('Weight', ascending=False)

plt.figure(figsize=(7.5, 4.5))
sns.barplot(x='Weight', y='Feature', data=feature_importance)
plt.title('Feature Importance')
plt.xlabel('Average Weight Magnitude')
plt.tight_layout()
plt.show()

# Save the best model
final_model.save('concrete_strength_prediction_model.h5')
print("\nModel saved as 'concrete_strength_prediction_model.h5'")

# Print summary of all results for comparison
print("\nModel Comparison Summary:")
print("=" * 80)
print(f"{'Configuration':<40} {'Val MSE':<10} {'Test MSE':<10} {'Test MAE':<10} {'Test R²':<10}")
print("-" * 80)
for i, result in enumerate(model_results):
    config = result['config']
    config_str = f"L:{config['n_layers']} A:{config['activation']} O:{config['optimizer']} LR:{config['lr_schedule']}"
    print(f"{config_str:<40} {result['validation_mse']:<10.4f} {result['test_mse']:<10.4f} {result['test_mae']:<10.4f} {result['test_r2']:<10.4f}")
print("=" * 80)
print(f"Baseline Model {'':<28} {baseline_mse:<10.4f} {baseline_mae:<10.4f} {baseline_r2:<10.4f}")
print(f"Final Model {'':<31} {final_mse:<10.4f} {final_mae:<10.4f} {final_r2:<10.4f}")
print("=" * 80)

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Load the saved model
model = load_model('/content/concrete_strength_prediction_model.h5')

# Load the dataset to get the scaler parameters and feature ranges
url = "https://docs.google.com/spreadsheets/d/1AYRqWP4AjJzR4TaERFq9XF2fiZi4tKOVyCUCYkuY3QQ/export?format=csv"
df = pd.read_csv(url)

# Identify the target column name
target_column = None
for col in df.columns:
    if 'compressive strength' in col.lower():
        target_column = col
        break
if target_column is None:
    target_column = df.columns[-1]  # Use last column as fallback

print(f"Target column: {target_column}")

# Extract features and target
X = df.drop(target_column, axis=1)
y = df[target_column]

# Get feature names and create a scaler
feature_names = X.columns
scaler = StandardScaler()
scaler.fit(X)

# Get the min and max values for each feature to use as bounds
feature_mins = X.min().values
feature_maxs = X.max().values

# Define a function to predict strength with the model
def predict_strength(mix_parameters):
    # Reshape parameters for scaler
    parameters_reshaped = np.array(mix_parameters).reshape(1, -1)
    # Scale parameters
    scaled_parameters = scaler.transform(parameters_reshaped)
    # Predict strength
    predicted_strength = model.predict(scaled_parameters, verbose=0)[0][0]
    return predicted_strength

# Define function that calculates difference between predicted and target strength
def objective_function(mix_parameters, target_strength):
    predicted = predict_strength(mix_parameters)
    return (predicted - target_strength) ** 2

# Function to find mix parameters for a given target strength
def find_mix_for_target_strength(target_strength, num_starts=5):
    best_solution = None
    best_error = float('inf')

    # Create bounds for the optimizer
    bounds = [(feature_mins[i], feature_maxs[i]) for i in range(len(feature_names))]

    for _ in range(num_starts):
        # Start with a random mix within bounds
        initial_guess = np.array([np.random.uniform(feature_mins[i], feature_maxs[i])
                                for i in range(len(feature_names))])

        # Run the optimization
        result = minimize(
            objective_function,
            initial_guess,
            args=(target_strength,),
            bounds=bounds,
            method='L-BFGS-B'
        )

        # Check if this solution is better
        if result.fun < best_error:
            best_error = result.fun
            best_solution = result.x

    # Calculate the predicted strength with the best solution
    predicted_strength = predict_strength(best_solution)

    return best_solution, predicted_strength, best_error

# Function to suggest multiple mix designs for a target strength
def suggest_mix_designs(target_strength, num_designs=3, num_starts_per_design=5):
    print(f"\nFinding mix designs for target strength: {target_strength:.2f} MPa\n")

    mix_designs = []

    for i in range(num_designs):
        mix_params, predicted_strength, error = find_mix_for_target_strength(
            target_strength,
            num_starts=num_starts_per_design
        )

        # Add small random perturbations to force a different solution next time
        feature_mins_adjusted = feature_mins * 1.05
        feature_maxs_adjusted = feature_maxs * 0.95

        mix_designs.append({
            'mix_parameters': mix_params,
            'predicted_strength': predicted_strength,
            'error': error
        })

        print(f"Design {i+1}:")
        for j, feature in enumerate(feature_names):
            print(f"  {feature}: {mix_params[j]:.2f}")
        print(f"  Predicted Strength: {predicted_strength:.2f} MPa")
        print(f"  Error: {error:.4f}")
        print()

    return mix_designs

# Function to create a practical concrete mix from model parameters
def create_practical_mix(mix_parameters):
    """Convert raw model parameters to a practical concrete mix design."""
    mix_dict = {name: value for name, value in zip(feature_names, mix_parameters)}

    # Calculate water-cement ratio
    if 'Cement (component 1)(kg in a m^3 mixture)' in mix_dict and 'Water (component 4)(kg in a m^3 mixture)' in mix_dict:
        cement = mix_dict.get('Cement (component 1)(kg in a m^3 mixture)', 0)
        water = mix_dict.get('Water (component 4)(kg in a m^3 mixture)', 0)
        if cement > 0:
            w_c_ratio = water / cement
        else:
            w_c_ratio = 0
    else:
        # Try to find columns by partial match
        cement_col = next((col for col in mix_dict if 'cement' in col.lower()), None)
        water_col = next((col for col in mix_dict if 'water' in col.lower()), None)

        if cement_col and water_col:
            cement = mix_dict[cement_col]
            water = mix_dict[water_col]
            if cement > 0:
                w_c_ratio = water / cement
            else:
                w_c_ratio = 0
        else:
            w_c_ratio = 0

    # Format the results
    practical_mix = {
        'Original Mix Parameters': {name: f"{value:.2f}" for name, value in zip(feature_names, mix_parameters)},
        'Water-Cement Ratio': f"{w_c_ratio:.3f}"
    }

    return practical_mix

# Interactive mode for users to input target strength
def interactive_mode():
    while True:
        try:
            print("\n" + "="*60)
            print("CONCRETE MIX DESIGN OPTIMIZER")
            print("="*60)

            target_strength = float(input("\nEnter your target compressive strength (MPa): "))

            # Get realistic range
            min_strength = y.min()
            max_strength = y.max()

            print(f"\nNote: Model was trained on strengths ranging from {min_strength:.2f} to {max_strength:.2f} MPa.")
            print("Values outside this range may produce less reliable results.")

            if target_strength < min_strength*0.7 or target_strength > max_strength*1.2:
                print("\nWARNING: Target strength is far outside the training range.")
                continue_anyway = input("Continue anyway? (y/n): ").lower()
                if continue_anyway != 'y':
                    continue

            num_designs = int(input("\nHow many alternative mix designs would you like? (1-5): "))
            num_designs = max(1, min(5, num_designs))  # Limit between 1 and 5

            mix_designs = suggest_mix_designs(target_strength, num_designs=num_designs)

            print("\nPractical Mix Information:")
            print("-"*60)

            for i, design in enumerate(mix_designs):
                practical_mix = create_practical_mix(design['mix_parameters'])

                print(f"\nMix Design {i+1}:")
                print(f"Predicted Strength: {design['predicted_strength']:.2f} MPa")
                print(f"Water-Cement Ratio: {practical_mix['Water-Cement Ratio']}")
                print("\nConcrete Mix Components (kg/m³):")

                for feature, value in practical_mix['Original Mix Parameters'].items():
                    print(f"  {feature}: {value}")

            another = input("\nWould you like to find another mix design? (y/n): ").lower()
            if another != 'y':
                break

        except ValueError:
            print("Invalid input. Please enter a numeric value for strength.")
        except Exception as e:
            print(f"An error occurred: {e}")

# Example usage with specific target strengths
def example_usage():
    print("\nExample 1: Standard strength concrete (30 MPa)")
    suggest_mix_designs(30.0, num_designs=2)

    print("\nExample 2: High strength concrete (50 MPa)")
    suggest_mix_designs(50.0, num_designs=2)

    print("\nExample 3: Very high strength concrete (70 MPa)")
    suggest_mix_designs(70.0, num_designs=2)

if __name__ == "__main__":
    print("\nCONCRETE MIX PARAMETER OPTIMIZER")
    print("This program uses a trained neural network model to find concrete mix")
    print("parameters that would achieve a target compressive strength.")

    choice = input("\nDo you want to see example results (e) or run in interactive mode (i)? ").lower()

    if choice == 'e':
        example_usage()
    else:
        interactive_mode()

